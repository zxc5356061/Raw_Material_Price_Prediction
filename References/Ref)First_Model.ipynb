{"cells":[{"source":"# One_month_prediction_Alkalis","metadata":{},"cell_type":"markdown","id":"413d7072-e17f-4c75-8250-7346f5c0ef19"},{"source":"## TO-DOs\n```\n[v] Import monthly electrcity data\n[v] Import monthly TTF_GAS data\n[v] Import price evaluatioin data\n[v] Create rows and encoding Alkalis_RM02_0001, Alkalis_RM02_0002\n[v] To calculate the monthly average prices of Alkalis\n[v] Create 12*N features, external factor prices from one-month before to 12-month before\n[v] Combine features with target variables\n[v] train_test_split() - do calculation and scaling only based on train data set to prevent data leakage\n[x] Detect outliers - skip\n[v] Check data distribution\n[v] Data scaling - log transformation\n[x] check multicollinearity(to run one regression using each features, and find corr of all feature, filtering those with higher performance and least corr for our last model) - skip\n[v] Lasso regression - fit and transform train data set\n[v] Cross validation and Hyperparameter tuning using RandomizedSearchCV\n[v] Lasso regression - transform test data set\n[] Compare Lasso with Simple linear model\n[] Visualisation\n```","metadata":{},"cell_type":"markdown","id":"6b4c7f02-1c48-419e-ae1c-979e205b1891"},{"source":"!pip install fredapi","metadata":{"executionCancelledAt":null,"executionTime":5204,"lastExecutedAt":1710453605427,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install fredapi","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"e79fdd88-9172-4097-9fa2-b91ab3eef2e5","outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: fredapi in /home/repl/.local/lib/python3.8/site-packages (0.5.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fredapi) (1.5.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (2022.7)\nRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (1.23.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->fredapi) (1.14.0)\n"}],"execution_count":2},{"source":"import pandas as pd\nfrom fredapi import Fred\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nimport numpy as np\nimport matplotlib.pyplot as plt\n","metadata":{"executionCancelledAt":null,"executionTime":3179,"lastExecutedAt":1710453608608,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom fredapi import Fred\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom numpy import inf\n"},"cell_type":"code","id":"d95bfcf0-14be-44fb-9a51-ceefcff17281","outputs":[],"execution_count":3},{"source":"def monthly_mean_to_daily(df_monthly: pd.core.frame.DataFrame ) -> pd.core.frame.DataFrame:\n    \"\"\"\n    Convert Monthly data into Daily data and impute with monthly mean prices\n    \"\"\"\n    df_monthly['Date'] = pd.to_datetime(df_monthly[['Year', 'Month']].assign(DAY=1))\n    df = df_monthly.explode('Date') # The explode() method converts each element of the specified column(s) into a row.\n\n    # Generate a complete range of daily dates for the year for imputation\n    start_date = df['Date'].min() # represents the starting point of your data\n    end_date = df['Date'].max() + pd.offsets.MonthEnd(1)  # finds the maximum (or latest) date and include the last month fully\n    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D') # generates a fixed-frequency DatetimeIndex\n\n    # Merge the full date range with the monthly averages to fill in all days\n    df_full_date_range = pd.DataFrame(full_date_range, columns=['Date'])\n    df = pd.merge(df_full_date_range, df_monthly, on='Date', how='left')\n    df_daily = df.ffill(axis=0) # to fill the missing value based on last valid observation following index sequence\n    return df_daily","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1710453608659,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def monthly_mean_to_daily(df_monthly: pd.core.frame.DataFrame ) -> pd.core.frame.DataFrame:\n    \"\"\"\n    Convert Monthly data into Daily data and impute with monthly mean prices\n    \"\"\"\n    df_monthly['Date'] = pd.to_datetime(df_monthly[['Year', 'Month']].assign(DAY=1))\n    df = df_monthly.explode('Date') # The explode() method converts each element of the specified column(s) into a row.\n\n    # Generate a complete range of daily dates for the year for imputation\n    start_date = df['Date'].min() # represents the starting point of your data\n    end_date = df['Date'].max() + pd.offsets.MonthEnd(1)  # finds the maximum (or latest) date and include the last month fully\n    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D') # generates a fixed-frequency DatetimeIndex\n\n    # Merge the full date range with the monthly averages to fill in all days\n    df_full_date_range = pd.DataFrame(full_date_range, columns=['Date'])\n    df = pd.merge(df_full_date_range, df_monthly, on='Date', how='left')\n    df_daily = df.ffill(axis=0) # to fill the missing value based on last valid observation following index sequence\n    return df_daily","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":true}},"cell_type":"code","id":"a8ebea83-36e8-4355-b23f-c3b4a33b8602","outputs":[],"execution_count":4},{"source":"## Import monthly electrcity data\nelec_df = pd.read_csv('ELECTRICITY.csv').iloc[:,1:]\nelec_df['Time'] = pd.to_datetime(elec_df['Year'].astype(str) + elec_df['Month'].astype(str), format='%Y%m')\nelec_df = elec_df[elec_df['Year'].between(2011,2023)].reset_index().drop('index',axis=1)\n\nprint(elec_df.info())\nprint(elec_df.groupby(['Year']).count())\nprint(elec_df.isna().sum().sort_values()) # checking missing values","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1710453608713,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Import monthly electrcity data\nelec_df = pd.read_csv('ELECTRICITY.csv').iloc[:,1:]\nelec_df['Time'] = pd.to_datetime(elec_df['Year'].astype(str) + elec_df['Month'].astype(str), format='%Y%m')\nelec_df = elec_df[elec_df['Year'].between(2011,2023)].reset_index().drop('index',axis=1)\n\nprint(elec_df.info())\nprint(elec_df.groupby(['Year']).count())\nprint(elec_df.isna().sum().sort_values()) # checking missing values","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"ca1221ea-d302-4c06-a97a-91017aa8eeea","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 156 entries, 0 to 155\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   Year         156 non-null    int64         \n 1   Month        156 non-null    int64         \n 2   Electricity  156 non-null    float64       \n 3   Time         156 non-null    datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(2)\nmemory usage: 5.0 KB\nNone\n      Month  Electricity  Time\nYear                          \n2011     12           12    12\n2012     12           12    12\n2013     12           12    12\n2014     12           12    12\n2015     12           12    12\n2016     12           12    12\n2017     12           12    12\n2018     12           12    12\n2019     12           12    12\n2020     12           12    12\n2021     12           12    12\n2022     12           12    12\n2023     12           12    12\nYear           0\nMonth          0\nElectricity    0\nTime           0\ndtype: int64\n"}],"execution_count":5},{"source":"## Import monthly TTF_GAS data\napiKey = '29219060bc68b2802af8584e0f328b52'\nfred = Fred(api_key=apiKey)\n\n# Get Natural Gas prices in Europe per month\nTTF_gas_df = pd.DataFrame(fred.get_series('PNGASEUUSDM'), \n                       columns=['PNGASEUUSDM']).reset_index() \nTTF_gas_df['index'] = pd.to_datetime(TTF_gas_df['index'], format='%Y-%m-%d')\nTTF_gas_df['Year'] = TTF_gas_df['index'].dt.year\nTTF_gas_df['Month'] = TTF_gas_df['index'].dt.month\nTTF_gas_df = TTF_gas_df[TTF_gas_df['Year'].between(2011,2023)]\nTTF_gas_df.rename(columns = {'index':'Time'}, inplace = True)\n\nprint(TTF_gas_df.info())\nprint(TTF_gas_df.groupby(['Year']).count())\nprint(TTF_gas_df.isna().sum().sort_values()) # Check missing values\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":294,"lastExecutedAt":1710453609007,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Import monthly TTF_GAS data\napiKey = '29219060bc68b2802af8584e0f328b52'\nfred = Fred(api_key=apiKey)\n\n# Get Natural Gas prices in Europe per month\nTTF_gas_df = pd.DataFrame(fred.get_series('PNGASEUUSDM'), \n                       columns=['PNGASEUUSDM']).reset_index() \nTTF_gas_df['index'] = pd.to_datetime(TTF_gas_df['index'], format='%Y-%m-%d')\nTTF_gas_df['Year'] = TTF_gas_df['index'].dt.year\nTTF_gas_df['Month'] = TTF_gas_df['index'].dt.month\nTTF_gas_df = TTF_gas_df[TTF_gas_df['Year'].between(2011,2023)]\nTTF_gas_df.rename(columns = {'index':'Time'}, inplace = True)\n\nprint(TTF_gas_df.info())\nprint(TTF_gas_df.groupby(['Year']).count())\nprint(TTF_gas_df.isna().sum().sort_values()) # Check missing values\n\n\n","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"b71f7113-0cfa-4a46-8130-9b07e18d20a9","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 156 entries, 312 to 467\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   Time         156 non-null    datetime64[ns]\n 1   PNGASEUUSDM  156 non-null    float64       \n 2   Year         156 non-null    int64         \n 3   Month        156 non-null    int64         \ndtypes: datetime64[ns](1), float64(1), int64(2)\nmemory usage: 6.1 KB\nNone\n      Time  PNGASEUUSDM  Month\nYear                          \n2011    12           12     12\n2012    12           12     12\n2013    12           12     12\n2014    12           12     12\n2015    12           12     12\n2016    12           12     12\n2017    12           12     12\n2018    12           12     12\n2019    12           12     12\n2020    12           12     12\n2021    12           12     12\n2022    12           12     12\n2023    12           12     12\nTime           0\nPNGASEUUSDM    0\nYear           0\nMonth          0\ndtype: int64\n"}],"execution_count":6},{"source":"## Import price evaluatioin data\nprice_evo_df = pd.read_csv('Dataset_Predicting_Price_Evolutions.csv').iloc[:,1:]\nprice_evo_df['POSTING DATE'] = pd.to_datetime(price_evo_df['POSTING DATE'], format='%Y-%m-%d')\nprice_evo_df['Year'] = price_evo_df['POSTING DATE'].dt.year\nprice_evo_df['Month'] = price_evo_df['POSTING DATE'].dt.month\n# price_evo_df = price_evo_df.sort_values(['Year','Month'],ascending=True)\nprice_evo_df = price_evo_df[price_evo_df['Year'].between(2012,2023)].reset_index().drop(['index'], axis=1)\n\n\nprice_evo_df.rename(columns = {'POSTING DATE':'Time'}, inplace = True)\n\n# Drop unnecessary columns\nprice_evo_df = price_evo_df.drop(['SITE', 'SUPPLIER NUMBER', 'PURCHASE NUMBER', 'WEIGHT (kg)'], axis=1)\n\nprint(price_evo_df.info())\nprint(price_evo_df.groupby(['Year']).count())\nprint(price_evo_df.isna().sum().sort_values()) # Check missing values\n\n","metadata":{"executionCancelledAt":null,"executionTime":103,"lastExecutedAt":1710453609110,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Import price evaluatioin data\nprice_evo_df = pd.read_csv('Dataset_Predicting_Price_Evolutions.csv').iloc[:,1:]\nprice_evo_df['POSTING DATE'] = pd.to_datetime(price_evo_df['POSTING DATE'], format='%Y-%m-%d')\nprice_evo_df['Year'] = price_evo_df['POSTING DATE'].dt.year\nprice_evo_df['Month'] = price_evo_df['POSTING DATE'].dt.month\n# price_evo_df = price_evo_df.sort_values(['Year','Month'],ascending=True)\nprice_evo_df = price_evo_df[price_evo_df['Year'].between(2012,2023)].reset_index().drop(['index'], axis=1)\n\n\nprice_evo_df.rename(columns = {'POSTING DATE':'Time'}, inplace = True)\n\n# Drop unnecessary columns\nprice_evo_df = price_evo_df.drop(['SITE', 'SUPPLIER NUMBER', 'PURCHASE NUMBER', 'WEIGHT (kg)'], axis=1)\n\nprint(price_evo_df.info())\nprint(price_evo_df.groupby(['Year']).count())\nprint(price_evo_df.isna().sum().sort_values()) # Check missing values\n\n","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"af06d94f-1218-4363-a83d-f5ac1783022a","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20570 entries, 0 to 20569\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   Time               20570 non-null  datetime64[ns]\n 1   Group Description  20570 non-null  object        \n 2   Key RM code        20570 non-null  object        \n 3   PRICE (EUR/kg)     20570 non-null  float64       \n 4   Year               20570 non-null  int64         \n 5   Month              20570 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(2), object(2)\nmemory usage: 964.3+ KB\nNone\n      Time  Group Description  Key RM code  PRICE (EUR/kg)  Month\nYear                                                             \n2012   604                604          604             604    604\n2013   634                634          634             634    634\n2014   803                803          803             803    803\n2015   860                860          860             860    860\n2016  1057               1057         1057            1057   1057\n2017  1339               1339         1339            1339   1339\n2018  1589               1589         1589            1589   1589\n2019  2151               2151         2151            2151   2151\n2020  2403               2403         2403            2403   2403\n2021  2954               2954         2954            2954   2954\n2022  3215               3215         3215            3215   3215\n2023  2961               2961         2961            2961   2961\nTime                 0\nGroup Description    0\nKey RM code          0\nPRICE (EUR/kg)       0\nYear                 0\nMonth                0\ndtype: int64\n"}],"execution_count":7},{"source":"## Create rows and encoding Alkalis_RM02_0001, \n## To calculate the monthly average prices of Alkalis\nAlkalis_df = price_evo_df[price_evo_df['Group Description']==\"Alkalis\"].sort_values(['Year','Month'],ascending=True)\nAlkalis_df = Alkalis_df.reset_index().drop('index',axis=1)\n\n# encoding Alkalis_RM02_0001, Alkalis_RM02_0002 with n-1 dummy variables\nAlkalis_df_dummies = pd.get_dummies(Alkalis_df['Key RM code'], drop_first=True)\nprint(Alkalis_df_dummies)\n# combine dummy variables with Alkalis_df\nAlkalis_df_dummies = pd.concat([Alkalis_df, Alkalis_df_dummies], axis=1)\nprint(Alkalis_df_dummies)\nAlkalis_df_dummies = Alkalis_df_dummies.drop('Key RM code', axis=1)\nprint(Alkalis_df_dummies)","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1710453609160,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Create rows and encoding Alkalis_RM02_0001, \n## To calculate the monthly average prices of Alkalis\nAlkalis_df = price_evo_df[price_evo_df['Group Description']==\"Alkalis\"].sort_values(['Year','Month'],ascending=True)\nAlkalis_df = Alkalis_df.reset_index().drop('index',axis=1)\n\n# encoding Alkalis_RM02_0001, Alkalis_RM02_0002 with n-1 dummy variables\nAlkalis_df_dummies = pd.get_dummies(Alkalis_df['Key RM code'], drop_first=True)\nprint(Alkalis_df_dummies)\n# combine dummy variables with Alkalis_df\nAlkalis_df_dummies = pd.concat([Alkalis_df, Alkalis_df_dummies], axis=1)\nprint(Alkalis_df_dummies)\nAlkalis_df_dummies = Alkalis_df_dummies.drop('Key RM code', axis=1)\nprint(Alkalis_df_dummies)","outputsMetadata":{"0":{"height":377,"type":"stream"}}},"cell_type":"code","id":"7e24f7f8-7f3e-4f64-9693-67c60f28a6b9","outputs":[{"output_type":"stream","name":"stdout","text":"      RM02/0002\n0             0\n1             0\n2             0\n3             0\n4             0\n...         ...\n6002          0\n6003          1\n6004          0\n6005          0\n6006          0\n\n[6007 rows x 1 columns]\n           Time Group Description Key RM code  ...  Year  Month  RM02/0002\n0    2012-01-31           Alkalis   RM02/0001  ...  2012      1          0\n1    2012-01-31           Alkalis   RM02/0001  ...  2012      1          0\n2    2012-01-31           Alkalis   RM02/0001  ...  2012      1          0\n3    2012-01-31           Alkalis   RM02/0001  ...  2012      1          0\n4    2012-01-31           Alkalis   RM02/0001  ...  2012      1          0\n...         ...               ...         ...  ...   ...    ...        ...\n6002 2023-10-03           Alkalis   RM02/0001  ...  2023     10          0\n6003 2023-10-03           Alkalis   RM02/0002  ...  2023     10          1\n6004 2023-10-03           Alkalis   RM02/0001  ...  2023     10          0\n6005 2023-10-02           Alkalis   RM02/0001  ...  2023     10          0\n6006 2023-10-02           Alkalis   RM02/0001  ...  2023     10          0\n\n[6007 rows x 7 columns]\n           Time Group Description  PRICE (EUR/kg)  Year  Month  RM02/0002\n0    2012-01-31           Alkalis        0.215000  2012      1          0\n1    2012-01-31           Alkalis        0.215000  2012      1          0\n2    2012-01-31           Alkalis        0.217000  2012      1          0\n3    2012-01-31           Alkalis        0.215000  2012      1          0\n4    2012-01-31           Alkalis        0.205000  2012      1          0\n...         ...               ...             ...   ...    ...        ...\n6002 2023-10-03           Alkalis        0.345890  2023     10          0\n6003 2023-10-03           Alkalis        0.654448  2023     10          1\n6004 2023-10-03           Alkalis        0.345890  2023     10          0\n6005 2023-10-02           Alkalis        0.345890  2023     10          0\n6006 2023-10-02           Alkalis        0.345890  2023     10          0\n\n[6007 rows x 6 columns]\n"}],"execution_count":8},{"source":"## Optional: To aggregate all observations with year, month, Key RM Code\n'''\nTotal data: 6007 -> 12 * No of year * No of key RM codes\n'''\nAlkalis_df_dummies['Time_index']=Alkalis_df_dummies['Time']\nAlkalis_df_dummies.set_index('Time_index', inplace=True)\n# Group by 'RM02/0002' and resample to monthly frequency while keeping the last value\nAlkalis_df_dummies = Alkalis_df_dummies.groupby(['RM02/0002'])\\\n                                        .resample('M')\\\n                                        .last()\\\n                                        .drop(['RM02/0002'],axis=1)\\\n                                        .reset_index()\\\n                                        .drop('Time_index',axis=1)\\\n                                        .dropna()\nprint(Alkalis_df_dummies.info())\n","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1710453609213,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Optional: To aggregate all observations with year, month, Key RM Code\n'''\nTotal data: 6007 -> 12 * No of year * No of key RM codes\n'''\nAlkalis_df_dummies['Time_index']=Alkalis_df_dummies['Time']\nAlkalis_df_dummies.set_index('Time_index', inplace=True)\n# Group by 'RM02/0002' and resample to monthly frequency while keeping the last value\nAlkalis_df_dummies = Alkalis_df_dummies.groupby(['RM02/0002'])\\\n                                        .resample('M')\\\n                                        .last()\\\n                                        .drop(['RM02/0002'],axis=1)\\\n                                        .reset_index()\\\n                                        .drop('Time_index',axis=1)\\\n                                        .dropna()\nprint(Alkalis_df_dummies.info())\n","outputsMetadata":{"0":{"height":337,"type":"stream"}}},"cell_type":"code","id":"a382947b-30f3-4118-a91b-585818d3fb26","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 282 entries, 0 to 282\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   RM02/0002          282 non-null    uint64        \n 1   Time               282 non-null    datetime64[ns]\n 2   Group Description  282 non-null    object        \n 3   PRICE (EUR/kg)     282 non-null    float64       \n 4   Year               282 non-null    float64       \n 5   Month              282 non-null    float64       \ndtypes: datetime64[ns](1), float64(3), object(1), uint64(1)\nmemory usage: 15.4+ KB\nNone\n"}],"execution_count":9},{"source":"## Calculate the average raw material price\naverage_price = Alkalis_df_dummies.groupby(['Year','Month','RM02/0002'])['PRICE (EUR/kg)']\\\n                                    .mean()\\\n                                    .reset_index()\n\n# Merge the average monthly price with the original dataframe\nAlkalis_df_dummies = pd.merge(Alkalis_df_dummies, average_price, on=['Year','Month','RM02/0002'], suffixes=('', '_avg'))\nprint(Alkalis_df_dummies.info())\n\n# Rename the new column to 'Average_price'\nAlkalis_df_dummies.rename(columns={'PRICE (EUR/kg)_avg': 'Average_price'}, inplace=True)\nAlkalis_df_dummies = Alkalis_df_dummies.drop('PRICE (EUR/kg)', axis=1)\n\nprint(Alkalis_df_dummies.info())\nprint(Alkalis_df_dummies.sort_values('Time'))\nprint(Alkalis_df_dummies.isna().sum().sort_values())\n","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1710453609260,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Calculate the average raw material price\naverage_price = Alkalis_df_dummies.groupby(['Year','Month','RM02/0002'])['PRICE (EUR/kg)']\\\n                                    .mean()\\\n                                    .reset_index()\n\n# Merge the average monthly price with the original dataframe\nAlkalis_df_dummies = pd.merge(Alkalis_df_dummies, average_price, on=['Year','Month','RM02/0002'], suffixes=('', '_avg'))\nprint(Alkalis_df_dummies.info())\n\n# Rename the new column to 'Average_price'\nAlkalis_df_dummies.rename(columns={'PRICE (EUR/kg)_avg': 'Average_price'}, inplace=True)\nAlkalis_df_dummies = Alkalis_df_dummies.drop('PRICE (EUR/kg)', axis=1)\n\nprint(Alkalis_df_dummies.info())\nprint(Alkalis_df_dummies.sort_values('Time'))\nprint(Alkalis_df_dummies.isna().sum().sort_values())\n","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"dcfaf48e-a15e-463f-9616-1a05c2f13f3e","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 282 entries, 0 to 281\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   RM02/0002           282 non-null    uint64        \n 1   Time                282 non-null    datetime64[ns]\n 2   Group Description   282 non-null    object        \n 3   PRICE (EUR/kg)      282 non-null    float64       \n 4   Year                282 non-null    float64       \n 5   Month               282 non-null    float64       \n 6   PRICE (EUR/kg)_avg  282 non-null    float64       \ndtypes: datetime64[ns](1), float64(4), object(1), uint64(1)\nmemory usage: 17.6+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 282 entries, 0 to 281\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   RM02/0002          282 non-null    uint64        \n 1   Time               282 non-null    datetime64[ns]\n 2   Group Description  282 non-null    object        \n 3   Year               282 non-null    float64       \n 4   Month              282 non-null    float64       \n 5   Average_price      282 non-null    float64       \ndtypes: datetime64[ns](1), float64(3), object(1), uint64(1)\nmemory usage: 15.4+ KB\nNone\n     RM02/0002       Time Group Description    Year  Month  Average_price\n0            0 2012-01-31           Alkalis  2012.0    1.0       0.215000\n142          1 2012-02-24           Alkalis  2012.0    2.0       0.410000\n1            0 2012-02-29           Alkalis  2012.0    2.0       0.215000\n2            0 2012-03-31           Alkalis  2012.0    3.0       0.215000\n143          1 2012-03-31           Alkalis  2012.0    3.0       0.410000\n..         ...        ...               ...     ...    ...            ...\n139          0 2023-08-31           Alkalis  2023.0    8.0       0.300000\n140          0 2023-09-30           Alkalis  2023.0    9.0       0.253164\n280          1 2023-09-30           Alkalis  2023.0    9.0       0.565000\n141          0 2023-10-31           Alkalis  2023.0   10.0       0.300000\n281          1 2023-10-31           Alkalis  2023.0   10.0       0.580000\n\n[282 rows x 6 columns]\nRM02/0002            0\nTime                 0\nGroup Description    0\nYear                 0\nMonth                0\nAverage_price        0\ndtype: int64\n"}],"execution_count":10},{"source":"## Calculate average prices based on Key RM Code\ntest = Alkalis_df_dummies.copy()\ntest = test.drop(['Time', 'Group Description', 'Year','Month'],axis=1)\nprint(test.groupby('RM02/0002')['Average_price'].mean())","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1710453609312,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Calculate average prices based on Key RM Code\ntest = Alkalis_df_dummies.copy()\ntest = test.drop(['Time', 'Group Description', 'Year','Month'],axis=1)\nprint(test.groupby('RM02/0002')['Average_price'].mean())","outputsMetadata":{"0":{"height":97,"type":"stream"}}},"cell_type":"code","id":"8acf6637-3c74-4e93-8fe8-f9ee62b313ba","outputs":[{"output_type":"stream","name":"stdout","text":"RM02/0002\n0    0.256541\n1    0.565323\nName: Average_price, dtype: float64\n"}],"execution_count":11},{"source":"## Create 12*N features, external factor prices from one-month before to 12-month before\n## Combine features with target variables\n# To prepare feature datasets\nmerged_df = pd.merge(elec_df, TTF_gas_df,how='left', on = (['Year', 'Month', 'Time']))\nfeature_df = merged_df.copy()\nfeature_df['Time_label'] = feature_df['Time'].dt.strftime('%Y-%m')\nfeature_df = feature_df.drop(['Year','Month', 'Time'], axis=1) # to prevent duplicate columns when merging\n# print(feature_df)\n\n# create time labels\nlabel_dfs=[]    # To store labels\n                # ref: 'https://pandas.pydata.org/docs/user_guide/merging.html'\n    \nfor i in range(1,13): # 13 is not included\n    label = Alkalis_df_dummies[['Time']]\n    label.rename(columns = {'Time':f'Time_label{i}'}, inplace = True)\n    label = (label[f'Time_label{i}'] - pd.DateOffset(months=i)).dt.strftime('%Y-%m')\n    label_dfs.append(label)\n\nresult = pd.concat(label_dfs, axis=1)\nprint(result)\n\n# To merge with features\nfor i in range(1,13): # 13 is not included\n    result = result.merge(feature_df, how='left',\\\n                          left_on=[f'Time_label{i}'],\\\n                          right_on=['Time_label'])\n    result.rename(columns = {'Electricity':f'Electricity_{i}',\n                              'PNGASEUUSDM':f'PNGASEUUSDM_{i}'\n                             }, inplace = True)\n    result = result.drop(['Time_label',f'Time_label{i}'], axis=1)\n    \nAlkalis_df_dummies = pd.concat([Alkalis_df_dummies,result],axis=1)\n# print(Alkalis_df_dummies.info())\nprint(Alkalis_df_dummies.info())\n\n","metadata":{"executionCancelledAt":null,"executionTime":82,"lastExecutedAt":1710453609394,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Create 12*N features, external factor prices from one-month before to 12-month before\n## Combine features with target variables\n# To prepare feature datasets\nmerged_df = pd.merge(elec_df, TTF_gas_df,how='left', on = (['Year', 'Month', 'Time']))\nfeature_df = merged_df.copy()\nfeature_df['Time_label'] = feature_df['Time'].dt.strftime('%Y-%m')\nfeature_df = feature_df.drop(['Year','Month', 'Time'], axis=1) # to prevent duplicate columns when merging\n# print(feature_df)\n\n# create time labels\nlabel_dfs=[]    # To store labels\n                # ref: 'https://pandas.pydata.org/docs/user_guide/merging.html'\n    \nfor i in range(1,13): # 13 is not included\n    label = Alkalis_df_dummies[['Time']]\n    label.rename(columns = {'Time':f'Time_label{i}'}, inplace = True)\n    label = (label[f'Time_label{i}'] - pd.DateOffset(months=i)).dt.strftime('%Y-%m')\n    label_dfs.append(label)\n\nresult = pd.concat(label_dfs, axis=1)\nprint(result)\n\n# To merge with features\nfor i in range(1,13): # 13 is not included\n    result = result.merge(feature_df, how='left',\\\n                          left_on=[f'Time_label{i}'],\\\n                          right_on=['Time_label'])\n    result.rename(columns = {'Electricity':f'Electricity_{i}',\n                              'PNGASEUUSDM':f'PNGASEUUSDM_{i}'\n                             }, inplace = True)\n    result = result.drop(['Time_label',f'Time_label{i}'], axis=1)\n    \nAlkalis_df_dummies = pd.concat([Alkalis_df_dummies,result],axis=1)\n# print(Alkalis_df_dummies.info())\nprint(Alkalis_df_dummies.info())\n\n","outputsMetadata":{"0":{"height":377,"type":"stream"},"1":{"height":217,"type":"stream"}}},"cell_type":"code","id":"af348660-2ae0-4781-bd13-8649ecd482d1","outputs":[{"output_type":"stream","name":"stdout","text":"    Time_label1 Time_label2 Time_label3  ... Time_label10 Time_label11 Time_label12\n0       2011-12     2011-11     2011-10  ...      2011-03      2011-02      2011-01\n1       2012-01     2011-12     2011-11  ...      2011-04      2011-03      2011-02\n2       2012-02     2012-01     2011-12  ...      2011-05      2011-04      2011-03\n3       2012-03     2012-02     2012-01  ...      2011-06      2011-05      2011-04\n4       2012-04     2012-03     2012-02  ...      2011-07      2011-06      2011-05\n..          ...         ...         ...  ...          ...          ...          ...\n277     2023-05     2023-04     2023-03  ...      2022-08      2022-07      2022-06\n278     2023-06     2023-05     2023-04  ...      2022-09      2022-08      2022-07\n279     2023-07     2023-06     2023-05  ...      2022-10      2022-09      2022-08\n280     2023-08     2023-07     2023-06  ...      2022-11      2022-10      2022-09\n281     2023-09     2023-08     2023-07  ...      2022-12      2022-11      2022-10\n\n[282 rows x 12 columns]\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 282 entries, 0 to 281\nData columns (total 30 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   RM02/0002          282 non-null    uint64        \n 1   Time               282 non-null    datetime64[ns]\n 2   Group Description  282 non-null    object        \n 3   Year               282 non-null    float64       \n 4   Month              282 non-null    float64       \n 5   Average_price      282 non-null    float64       \n 6   Electricity_1      282 non-null    float64       \n 7   PNGASEUUSDM_1      282 non-null    float64       \n 8   Electricity_2      282 non-null    float64       \n 9   PNGASEUUSDM_2      282 non-null    float64       \n 10  Electricity_3      282 non-null    float64       \n 11  PNGASEUUSDM_3      282 non-null    float64       \n 12  Electricity_4      282 non-null    float64       \n 13  PNGASEUUSDM_4      282 non-null    float64       \n 14  Electricity_5      282 non-null    float64       \n 15  PNGASEUUSDM_5      282 non-null    float64       \n 16  Electricity_6      282 non-null    float64       \n 17  PNGASEUUSDM_6      282 non-null    float64       \n 18  Electricity_7      282 non-null    float64       \n 19  PNGASEUUSDM_7      282 non-null    float64       \n 20  Electricity_8      282 non-null    float64       \n 21  PNGASEUUSDM_8      282 non-null    float64       \n 22  Electricity_9      282 non-null    float64       \n 23  PNGASEUUSDM_9      282 non-null    float64       \n 24  Electricity_10     282 non-null    float64       \n 25  PNGASEUUSDM_10     282 non-null    float64       \n 26  Electricity_11     282 non-null    float64       \n 27  PNGASEUUSDM_11     282 non-null    float64       \n 28  Electricity_12     282 non-null    float64       \n 29  PNGASEUUSDM_12     282 non-null    float64       \ndtypes: datetime64[ns](1), float64(27), object(1), uint64(1)\nmemory usage: 68.3+ KB\nNone\n"}],"execution_count":12},{"source":"## train_test_split()\n## Log transformation\n\n# # Observe data distribution\n# Alkalis_df_dummies.drop(['RM02/0002','Time', 'Group Description', 'Year','Month'],axis=1).hist()\n# Alkalis_df_dummies['Average_price'].hist()\n\n# Create X, y\nfeature_list = Alkalis_df_dummies.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\nX = feature_list.values\ny = Alkalis_df_dummies['Average_price'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 30% of our data as the test set\n\n# Log transformation and standardlisation\ny_train_log = np.log(y_train)\ny_test_log = np.log(y_test)\n\nscaler_x = StandardScaler()\nX_train_scaled = scaler_x.fit_transform(X_train)\nX_test_scaled = scaler_x.transform(X_test)\n\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train_log.reshape(-1,1))\ny_test_scaled = scaler_y.transform(y_test_log.reshape(-1,1))","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1710453609443,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## train_test_split()\n## Log transformation\n\n# # Observe data distribution\n# Alkalis_df_dummies.drop(['RM02/0002','Time', 'Group Description', 'Year','Month'],axis=1).hist()\n# Alkalis_df_dummies['Average_price'].hist()\n\n# Create X, y\nfeature_list = Alkalis_df_dummies.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\nX = feature_list.values\ny = Alkalis_df_dummies['Average_price'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 30% of our data as the test set\n\n# Log transformation and standardlisation\ny_train_log = np.log(y_train)\ny_test_log = np.log(y_test)\n\nscaler_x = StandardScaler()\nX_train_scaled = scaler_x.fit_transform(X_train)\nX_test_scaled = scaler_x.transform(X_test)\n\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train_log.reshape(-1,1))\ny_test_scaled = scaler_y.transform(y_test_log.reshape(-1,1))","outputsMetadata":{"0":{"height":157,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"48e7f43f-d67c-4079-a16e-4cc3f57de35e","outputs":[],"execution_count":13},{"source":"## Lasso regression - fit and transform train data set\n## Cross validation and Hyperparameter tuning using RandomizedSearchCV\n\n# Define the parameter grid\nparam_grid = {'alpha': np.linspace(0.0000001, 1, 3000)}\n\n# Create a Lasso regression model\nlasso = Lasso()\n\n# Create RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=lasso, \n                                   param_distributions=param_grid, \n                                   n_iter=300, \n                                   cv=5, \n                                   random_state=42)\n\n# Fit the data to perform a grid search\nrandom_search.fit(X_train_scaled, y_train_scaled)\n\n# Best alpha parameter\nprint(\"Best alpha parameter:\", random_search.best_params_['alpha'])\n\n# Best R-squared score\nprint(\"Best R-squared score:\", round(random_search.best_score_, 3))\n\n# Coefficients of the best Lasso model\nassert random_search.n_features_in_ == len(feature_list.columns)\n\nprint(\"Coefficients of the selected features in the best Lasso model:\")\nfor feature, coefficient in zip(feature_list.columns, random_search.best_estimator_.coef_):\n    print(f\"{feature}: {round(coefficient,3)}\")","metadata":{"executionCancelledAt":null,"executionTime":1616,"lastExecutedAt":1710453611060,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Lasso regression - fit and transform train data set\n## Cross validation and Hyperparameter tuning using RandomizedSearchCV\n\n# Define the parameter grid\nparam_grid = {'alpha': np.linspace(0.0000001, 1, 3000)}\n\n# Create a Lasso regression model\nlasso = Lasso()\n\n# Create RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=lasso, \n                                   param_distributions=param_grid, \n                                   n_iter=300, \n                                   cv=5, \n                                   random_state=42)\n\n# Fit the data to perform a grid search\nrandom_search.fit(X_train_scaled, y_train_scaled)\n\n# Best alpha parameter\nprint(\"Best alpha parameter:\", random_search.best_params_['alpha'])\n\n# Best R-squared score\nprint(\"Best R-squared score:\", round(random_search.best_score_, 3))\n\n# Coefficients of the best Lasso model\nassert random_search.n_features_in_ == len(feature_list.columns)\n\nprint(\"Coefficients of the selected features in the best Lasso model:\")\nfor feature, coefficient in zip(feature_list.columns, random_search.best_estimator_.coef_):\n    print(f\"{feature}: {round(coefficient,3)}\")","outputsMetadata":{"0":{"height":377,"type":"stream"}}},"cell_type":"code","id":"b8215f5e-fb58-41ec-a76f-10a7cc271c58","outputs":[{"output_type":"stream","name":"stdout","text":"Best alpha parameter: 0.01700576685561854\nBest R-squared score: 0.881\nCoefficients of the selected features in the best Lasso model:\nRM02/0002: 0.689\nElectricity_1: 0.28\nPNGASEUUSDM_1: 0.109\nElectricity_2: 0.08\nPNGASEUUSDM_2: 0.0\nElectricity_3: 0.099\nPNGASEUUSDM_3: 0.0\nElectricity_4: 0.0\nPNGASEUUSDM_4: 0.0\nElectricity_5: 0.0\nPNGASEUUSDM_5: 0.0\nElectricity_6: 0.076\nPNGASEUUSDM_6: 0.0\nElectricity_7: 0.0\nPNGASEUUSDM_7: 0.0\nElectricity_8: 0.083\nPNGASEUUSDM_8: -0.0\nElectricity_9: 0.0\nPNGASEUUSDM_9: -0.0\nElectricity_10: 0.0\nPNGASEUUSDM_10: 0.0\nElectricity_11: 0.0\nPNGASEUUSDM_11: -0.0\nElectricity_12: 0.0\nPNGASEUUSDM_12: -0.0\n"}],"execution_count":14},{"source":"## Lasso regression - transform test data set\n# Get the best Lasso model from RandomizedSearchCV\nbest_lasso_model = random_search.best_estimator_\n\n# Predict on the test data\ny_pred_test = best_lasso_model.predict(X_test_scaled)\n\n# Evaluate the model performance on the test data\ntest_score = best_lasso_model.score(X_test_scaled, y_test_scaled)\nprint(\"Best Model:\", best_lasso_model)\nprint(\"Test Set R-squared score:\", round(test_score, 3))\n\n","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1710453611108,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Lasso regression - transform test data set\n# Get the best Lasso model from RandomizedSearchCV\nbest_lasso_model = random_search.best_estimator_\n\n# Predict on the test data\ny_pred_test = best_lasso_model.predict(X_test_scaled)\n\n# Evaluate the model performance on the test data\ntest_score = best_lasso_model.score(X_test_scaled, y_test_scaled)\nprint(\"Best Model:\", best_lasso_model)\nprint(\"Test Set R-squared score:\", round(test_score, 3))\n\n","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"cell_type":"code","id":"49a8e9d1-f461-4266-981f-9edd99e8fd77","outputs":[{"output_type":"stream","name":"stdout","text":"Best Model: Lasso(alpha=0.01700576685561854)\nTest Set R-squared score: 0.871\n"}],"execution_count":15},{"source":"## Simple Linear regression - fit and transform train data set\n## Cross validation and Hyperparameter tuning using RandomizedSearchCV\n\n# Define the parameter grid\nparam_grid = {'fit_intercept': [True, False]}\n\n# Create a Lasso regression model\nlinear = LinearRegression()\n\n# Create RandomizedSearchCV object\nrandom_search_compare = RandomizedSearchCV(estimator=linear, \n                                   param_distributions=param_grid, \n                                   n_iter=300, \n                                   cv=5, \n                                   random_state=42)\n\n# Fit the data to perform a grid search\nrandom_search_compare.fit(X_train_scaled, y_train_scaled)\n\n# Best alpha parameter\nprint(\"Best parameter:\", random_search_compare.best_params_['fit_intercept'])\n\n# Best R-squared score\nprint(\"Best R-squared score:\", round(random_search_compare.best_score_, 3))\n\n# Coefficients of the best Lasso model\nassert random_search_compare.n_features_in_ == len(feature_list.columns)\n\nprint(\"Coefficients of the selected features in the best Linear model:\")\nfor feature, coefficient in zip(feature_list.columns, random_search_compare.best_estimator_.coef_):\n    print(f\"{feature}: {np.round(coefficient,3)}\")","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1710453611160,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Simple Linear regression - fit and transform train data set\n## Cross validation and Hyperparameter tuning using RandomizedSearchCV\n\n# Define the parameter grid\nparam_grid = {'fit_intercept': [True, False]}\n\n# Create a Lasso regression model\nlinear = LinearRegression()\n\n# Create RandomizedSearchCV object\nrandom_search_compare = RandomizedSearchCV(estimator=linear, \n                                   param_distributions=param_grid, \n                                   n_iter=300, \n                                   cv=5, \n                                   random_state=42)\n\n# Fit the data to perform a grid search\nrandom_search_compare.fit(X_train_scaled, y_train_scaled)\n\n# Best alpha parameter\nprint(\"Best parameter:\", random_search_compare.best_params_['fit_intercept'])\n\n# Best R-squared score\nprint(\"Best R-squared score:\", round(random_search_compare.best_score_, 3))\n\n# Coefficients of the best Lasso model\nassert random_search_compare.n_features_in_ == len(feature_list.columns)\n\nprint(\"Coefficients of the selected features in the best Linear model:\")\nfor feature, coefficient in zip(feature_list.columns, random_search_compare.best_estimator_.coef_):\n    print(f\"{feature}: {np.round(coefficient,3)}\")","outputsMetadata":{"0":{"height":197,"type":"stream"}}},"cell_type":"code","id":"f2598b81-d6cf-471b-9492-03b8e3b34575","outputs":[{"output_type":"stream","name":"stdout","text":"Best parameter: False\nBest R-squared score: 0.77\nCoefficients of the selected features in the best Linear model:\nRM02/0002: [ 0.704  0.299  0.142 -0.194 -0.113  0.385  0.138 -0.042  0.077 -0.137\n -0.119  0.234  0.085 -0.     0.068  0.087  0.034 -0.042 -0.253  0.264\n  0.354  0.046 -0.288 -0.132 -0.176]\n"}],"execution_count":16},{"source":"## Simple Linear regression - transform test data set\n# Get the best Lasso model from RandomizedSearchCV\nbest_linear_model = random_search_compare.best_estimator_\n\n# Predict on the test data\ny_pred_test = best_linear_model.predict(X_test_scaled)\n\n# Evaluate the model performance on the test data\ntest_score = best_lasso_model.score(X_test_scaled, y_test_scaled)\nprint(\"Best Model:\", best_linear_model)\nprint(\"Test Set R-squared score:\", round(test_score, 3))\n\n","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1710453611207,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Simple Linear regression - transform test data set\n# Get the best Lasso model from RandomizedSearchCV\nbest_linear_model = random_search_compare.best_estimator_\n\n# Predict on the test data\ny_pred_test = best_linear_model.predict(X_test_scaled)\n\n# Evaluate the model performance on the test data\ntest_score = best_lasso_model.score(X_test_scaled, y_test_scaled)\nprint(\"Best Model:\", best_linear_model)\nprint(\"Test Set R-squared score:\", round(test_score, 3))\n\n","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"cell_type":"code","id":"e8f6a89c-689a-4b81-a060-37af2fea9950","outputs":[{"output_type":"stream","name":"stdout","text":"Best Model: LinearRegression(fit_intercept=False)\nTest Set R-squared score: 0.871\n"}],"execution_count":17}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}