{"cells":[{"source":"# One_month_prediction_Alkalis","metadata":{},"cell_type":"markdown","id":"413d7072-e17f-4c75-8250-7346f5c0ef19"},{"source":"## TO-DOs\n```\n[v] Import monthly electrcity data\n[v] Import monthly TTF_GAS data\n[v] Import price evaluatioin data\n[v] Create rows and encoding Alkalis_RM02_0001, Alkalis_RM02_0002\n[v] To calculate the monthly average prices of Alkalis\n[v] Create 12*N features, external factor prices from one-month before to 12-month before\n[v] Combine features with target variables\n[v] train_test_split() - do calculation and scaling only based on train data set to prevent data leakage\n[x] Detect outliers - skip\n[v] Check data distribution\n[v] Data scaling - log transformation\n[x] check multicollinearity(to run one regression using each features, and find corr of all feature, filtering those with higher performance and least corr for our last model) - skip\n[v] Lasso regression - fit and transform train data set\n[v] Cross validation and Hyperparameter tuning using RandomizedSearchCV\n[v] Lasso regression - transform test data set\n```","metadata":{},"cell_type":"markdown","id":"6b4c7f02-1c48-419e-ae1c-979e205b1891"},{"source":"!pip install fredapi\n!pip install pandasql","metadata":{"executionCancelledAt":null,"executionTime":11142,"lastExecutedAt":1710238481181,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install fredapi\n!pip install pandasql","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"e79fdd88-9172-4097-9fa2-b91ab3eef2e5","outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: fredapi in /home/repl/.local/lib/python3.8/site-packages (0.5.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fredapi) (1.5.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (2022.7)\nRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (1.23.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->fredapi) (1.14.0)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandasql in /usr/local/lib/python3.8/dist-packages (0.7.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pandasql) (1.23.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from pandasql) (1.5.1)\nRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.8/dist-packages (from pandasql) (1.4.40)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->pandasql) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->pandasql) (2022.7)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy->pandasql) (1.1.3)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->pandasql) (1.14.0)\n"}],"execution_count":1},{"source":"import pandas as pd\nfrom fredapi import Fred\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"executionCancelledAt":null,"executionTime":684,"lastExecutedAt":1710238481866,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom fredapi import Fred\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\nimport matplotlib.pyplot as plt"},"cell_type":"code","id":"d95bfcf0-14be-44fb-9a51-ceefcff17281","outputs":[],"execution_count":2},{"source":"def monthly_mean_to_daily(df_monthly: pd.core.frame.DataFrame ) -> pd.core.frame.DataFrame:\n    \"\"\"\n    Convert Monthly data into Daily data and impute with monthly mean prices\n    \"\"\"\n    df_monthly['Date'] = pd.to_datetime(df_monthly[['Year', 'Month']].assign(DAY=1))\n    df = df_monthly.explode('Date') # The explode() method converts each element of the specified column(s) into a row.\n\n    # Generate a complete range of daily dates for the year for imputation\n    start_date = df['Date'].min() # represents the starting point of your data\n    end_date = df['Date'].max() + pd.offsets.MonthEnd(1)  # finds the maximum (or latest) date and include the last month fully\n    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D') # generates a fixed-frequency DatetimeIndex\n\n    # Merge the full date range with the monthly averages to fill in all days\n    df_full_date_range = pd.DataFrame(full_date_range, columns=['Date'])\n    df = pd.merge(df_full_date_range, df_monthly, on='Date', how='left')\n    df_daily = df.ffill(axis=0) # to fill the missing value based on last valid observation following index sequence\n    return df_daily","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1710238481920,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def monthly_mean_to_daily(df_monthly: pd.core.frame.DataFrame ) -> pd.core.frame.DataFrame:\n    \"\"\"\n    Convert Monthly data into Daily data and impute with monthly mean prices\n    \"\"\"\n    df_monthly['Date'] = pd.to_datetime(df_monthly[['Year', 'Month']].assign(DAY=1))\n    df = df_monthly.explode('Date') # The explode() method converts each element of the specified column(s) into a row.\n\n    # Generate a complete range of daily dates for the year for imputation\n    start_date = df['Date'].min() # represents the starting point of your data\n    end_date = df['Date'].max() + pd.offsets.MonthEnd(1)  # finds the maximum (or latest) date and include the last month fully\n    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D') # generates a fixed-frequency DatetimeIndex\n\n    # Merge the full date range with the monthly averages to fill in all days\n    df_full_date_range = pd.DataFrame(full_date_range, columns=['Date'])\n    df = pd.merge(df_full_date_range, df_monthly, on='Date', how='left')\n    df_daily = df.ffill(axis=0) # to fill the missing value based on last valid observation following index sequence\n    return df_daily","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":true}},"cell_type":"code","id":"a8ebea83-36e8-4355-b23f-c3b4a33b8602","outputs":[],"execution_count":3},{"source":"## Import monthly electrcity data\nelec_df = pd.read_csv('ELECTRICITY.csv').iloc[:,1:]\nelec_df['Time'] = pd.to_datetime(elec_df['Year'].astype(str) + elec_df['Month'].astype(str), format='%Y%m')\nelec_df = elec_df[elec_df['Year'].between(2011,2023)].reset_index().drop('index',axis=1)\n\nprint(elec_df.info())\nprint(elec_df.groupby(['Year']).count())\nprint(elec_df.isna().sum().sort_values()) # checking missing values","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1710238481972,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Import monthly electrcity data\nelec_df = pd.read_csv('ELECTRICITY.csv').iloc[:,1:]\nelec_df['Time'] = pd.to_datetime(elec_df['Year'].astype(str) + elec_df['Month'].astype(str), format='%Y%m')\nelec_df = elec_df[elec_df['Year'].between(2011,2023)].reset_index().drop('index',axis=1)\n\nprint(elec_df.info())\nprint(elec_df.groupby(['Year']).count())\nprint(elec_df.isna().sum().sort_values()) # checking missing values","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"ca1221ea-d302-4c06-a97a-91017aa8eeea","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 156 entries, 0 to 155\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   Year         156 non-null    int64         \n 1   Month        156 non-null    int64         \n 2   Electricity  156 non-null    float64       \n 3   Time         156 non-null    datetime64[ns]\ndtypes: datetime64[ns](1), float64(1), int64(2)\nmemory usage: 5.0 KB\nNone\n      Month  Electricity  Time\nYear                          \n2011     12           12    12\n2012     12           12    12\n2013     12           12    12\n2014     12           12    12\n2015     12           12    12\n2016     12           12    12\n2017     12           12    12\n2018     12           12    12\n2019     12           12    12\n2020     12           12    12\n2021     12           12    12\n2022     12           12    12\n2023     12           12    12\nYear           0\nMonth          0\nElectricity    0\nTime           0\ndtype: int64\n"}],"execution_count":4},{"source":"## Import monthly TTF_GAS data\napiKey = '29219060bc68b2802af8584e0f328b52'\nfred = Fred(api_key=apiKey)\n\n# Get Natural Gas prices in Europe per month\nTTF_gas_df = pd.DataFrame(fred.get_series('PNGASEUUSDM'), \n                       columns=['PNGASEUUSDM']).reset_index() \nTTF_gas_df['index'] = pd.to_datetime(TTF_gas_df['index'], format='%Y-%m-%d')\nTTF_gas_df['Year'] = TTF_gas_df['index'].dt.year\nTTF_gas_df['Month'] = TTF_gas_df['index'].dt.month\nTTF_gas_df = TTF_gas_df[TTF_gas_df['Year'].between(2011,2023)]\nTTF_gas_df.rename(columns = {'index':'Time'}, inplace = True)\n\nprint(TTF_gas_df.info())\nprint(TTF_gas_df.groupby(['Year']).count())\nprint(TTF_gas_df.isna().sum().sort_values()) # Check missing values\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":183,"lastExecutedAt":1710238482155,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Import monthly TTF_GAS data\napiKey = '29219060bc68b2802af8584e0f328b52'\nfred = Fred(api_key=apiKey)\n\n# Get Natural Gas prices in Europe per month\nTTF_gas_df = pd.DataFrame(fred.get_series('PNGASEUUSDM'), \n                       columns=['PNGASEUUSDM']).reset_index() \nTTF_gas_df['index'] = pd.to_datetime(TTF_gas_df['index'], format='%Y-%m-%d')\nTTF_gas_df['Year'] = TTF_gas_df['index'].dt.year\nTTF_gas_df['Month'] = TTF_gas_df['index'].dt.month\nTTF_gas_df = TTF_gas_df[TTF_gas_df['Year'].between(2011,2023)]\nTTF_gas_df.rename(columns = {'index':'Time'}, inplace = True)\n\nprint(TTF_gas_df.info())\nprint(TTF_gas_df.groupby(['Year']).count())\nprint(TTF_gas_df.isna().sum().sort_values()) # Check missing values\n\n\n","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"b71f7113-0cfa-4a46-8130-9b07e18d20a9","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 156 entries, 312 to 467\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   Time         156 non-null    datetime64[ns]\n 1   PNGASEUUSDM  156 non-null    float64       \n 2   Year         156 non-null    int64         \n 3   Month        156 non-null    int64         \ndtypes: datetime64[ns](1), float64(1), int64(2)\nmemory usage: 6.1 KB\nNone\n      Time  PNGASEUUSDM  Month\nYear                          \n2011    12           12     12\n2012    12           12     12\n2013    12           12     12\n2014    12           12     12\n2015    12           12     12\n2016    12           12     12\n2017    12           12     12\n2018    12           12     12\n2019    12           12     12\n2020    12           12     12\n2021    12           12     12\n2022    12           12     12\n2023    12           12     12\nTime           0\nPNGASEUUSDM    0\nYear           0\nMonth          0\ndtype: int64\n"}],"execution_count":5},{"source":"## Import price evaluatioin data\nprice_evo_df = pd.read_csv('Dataset_Predicting_Price_Evolutions.csv').iloc[:,1:]\nprice_evo_df['POSTING DATE'] = pd.to_datetime(price_evo_df['POSTING DATE'], format='%Y-%m-%d')\nprice_evo_df['Year'] = price_evo_df['POSTING DATE'].dt.year\nprice_evo_df['Month'] = price_evo_df['POSTING DATE'].dt.month\n# price_evo_df = price_evo_df.sort_values(['Year','Month'],ascending=True)\nprice_evo_df = price_evo_df[price_evo_df['Year'].between(2012,2023)].reset_index().drop(['index'], axis=1)\n\n\nprice_evo_df.rename(columns = {'POSTING DATE':'Time'}, inplace = True)\n\n# Drop unnecessary columns\nprice_evo_df = price_evo_df.drop(['SITE', 'SUPPLIER NUMBER', 'PURCHASE NUMBER', 'WEIGHT (kg)'], axis=1)\n\nprint(price_evo_df.info())\nprint(price_evo_df.groupby(['Year']).count())\nprint(price_evo_df.isna().sum().sort_values()) # Check missing values\n\n","metadata":{"executionCancelledAt":null,"executionTime":78,"lastExecutedAt":1710238482233,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Import price evaluatioin data\nprice_evo_df = pd.read_csv('Dataset_Predicting_Price_Evolutions.csv').iloc[:,1:]\nprice_evo_df['POSTING DATE'] = pd.to_datetime(price_evo_df['POSTING DATE'], format='%Y-%m-%d')\nprice_evo_df['Year'] = price_evo_df['POSTING DATE'].dt.year\nprice_evo_df['Month'] = price_evo_df['POSTING DATE'].dt.month\n# price_evo_df = price_evo_df.sort_values(['Year','Month'],ascending=True)\nprice_evo_df = price_evo_df[price_evo_df['Year'].between(2012,2023)].reset_index().drop(['index'], axis=1)\n\n\nprice_evo_df.rename(columns = {'POSTING DATE':'Time'}, inplace = True)\n\n# Drop unnecessary columns\nprice_evo_df = price_evo_df.drop(['SITE', 'SUPPLIER NUMBER', 'PURCHASE NUMBER', 'WEIGHT (kg)'], axis=1)\n\nprint(price_evo_df.info())\nprint(price_evo_df.groupby(['Year']).count())\nprint(price_evo_df.isna().sum().sort_values()) # Check missing values\n\n","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"af06d94f-1218-4363-a83d-f5ac1783022a","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20570 entries, 0 to 20569\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   Time               20570 non-null  datetime64[ns]\n 1   Group Description  20570 non-null  object        \n 2   Key RM code        20570 non-null  object        \n 3   PRICE (EUR/kg)     20570 non-null  float64       \n 4   Year               20570 non-null  int64         \n 5   Month              20570 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(2), object(2)\nmemory usage: 964.3+ KB\nNone\n      Time  Group Description  Key RM code  PRICE (EUR/kg)  Month\nYear                                                             \n2012   604                604          604             604    604\n2013   634                634          634             634    634\n2014   803                803          803             803    803\n2015   860                860          860             860    860\n2016  1057               1057         1057            1057   1057\n2017  1339               1339         1339            1339   1339\n2018  1589               1589         1589            1589   1589\n2019  2151               2151         2151            2151   2151\n2020  2403               2403         2403            2403   2403\n2021  2954               2954         2954            2954   2954\n2022  3215               3215         3215            3215   3215\n2023  2961               2961         2961            2961   2961\nTime                 0\nGroup Description    0\nKey RM code          0\nPRICE (EUR/kg)       0\nYear                 0\nMonth                0\ndtype: int64\n"}],"execution_count":6},{"source":"## Create rows and encoding Alkalis_RM02_0001, \n## To calculate the monthly average prices of Alkalis\nAlkalis_df = price_evo_df[price_evo_df['Group Description']==\"Alkalis\"].sort_values(['Year','Month'],ascending=True)\nAlkalis_df = Alkalis_df.reset_index().drop('index',axis=1)\n\n# encoding Alkalis_RM02_0001, Alkalis_RM02_0002 with n-1 dummy variables\nAlkalis_df_dummies = pd.get_dummies(Alkalis_df['Key RM code'], drop_first=True)\n# combine dummy variables with Alkalis_df\nAlkalis_df_dummies = pd.concat([Alkalis_df, Alkalis_df_dummies], axis=1)\nAlkalis_df_dummies = Alkalis_df_dummies.drop('Key RM code', axis=1)\n\n\n## Calculate the average raw material price\naverage_price = Alkalis_df_dummies.groupby(['Year','Month','RM02/0002'])['PRICE (EUR/kg)']\\\n                                    .mean()\\\n                                    .reset_index()\n\n# Merge the average monthly price with the original dataframe\nAlkalis_df_dummies = pd.merge(Alkalis_df_dummies, average_price, on=['Year','Month','RM02/0002'], suffixes=('', '_avg'))\nprint(Alkalis_df_dummies)\n\n# Rename the new column to 'Average_price'\nAlkalis_df_dummies.rename(columns={'PRICE (EUR/kg)_avg': 'Average_price'}, inplace=True)\nAlkalis_df_dummies = Alkalis_df_dummies.drop('PRICE (EUR/kg)', axis=1)\n\nprint(Alkalis_df_dummies.info())\nprint(Alkalis_df_dummies.sort_values('Time'))\nprint(Alkalis_df_dummies.isna().sum().sort_values())\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"dcfaf48e-a15e-463f-9616-1a05c2f13f3e","outputs":[{"output_type":"stream","name":"stdout","text":"           Time Group Description  ...  RM02/0002  PRICE (EUR/kg)_avg\n0    2012-01-31           Alkalis  ...          0            0.214250\n1    2012-01-31           Alkalis  ...          0            0.214250\n2    2012-01-31           Alkalis  ...          0            0.214250\n3    2012-01-31           Alkalis  ...          0            0.214250\n4    2012-01-31           Alkalis  ...          0            0.214250\n...         ...               ...  ...        ...                 ...\n6002 2023-10-19           Alkalis  ...          1            0.612099\n6003 2023-10-19           Alkalis  ...          1            0.612099\n6004 2023-10-11           Alkalis  ...          1            0.612099\n6005 2023-10-05           Alkalis  ...          1            0.612099\n6006 2023-10-03           Alkalis  ...          1            0.612099\n\n[6007 rows x 7 columns]\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 6007 entries, 0 to 6006\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   Time               6007 non-null   datetime64[ns]\n 1   Group Description  6007 non-null   object        \n 2   Year               6007 non-null   int64         \n 3   Month              6007 non-null   int64         \n 4   RM02/0002          6007 non-null   uint8         \n 5   Average_price      6007 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), int64(2), object(1), uint8(1)\nmemory usage: 287.4+ KB\nNone\n           Time Group Description  Year  Month  RM02/0002  Average_price\n0    2012-01-31           Alkalis  2012      1          0       0.214250\n1    2012-01-31           Alkalis  2012      1          0       0.214250\n2    2012-01-31           Alkalis  2012      1          0       0.214250\n3    2012-01-31           Alkalis  2012      1          0       0.214250\n4    2012-01-31           Alkalis  2012      1          0       0.214250\n...         ...               ...   ...    ...        ...            ...\n5929 2023-10-31           Alkalis  2023     10          0       0.327462\n5930 2023-10-31           Alkalis  2023     10          0       0.327462\n5931 2023-10-31           Alkalis  2023     10          0       0.327462\n5920 2023-10-31           Alkalis  2023     10          0       0.327462\n5925 2023-10-31           Alkalis  2023     10          0       0.327462\n\n[6007 rows x 6 columns]\nTime                 0\nGroup Description    0\nYear                 0\nMonth                0\nRM02/0002            0\nAverage_price        0\ndtype: int64\n"}],"execution_count":7},{"source":"## Calculate average prices based on Key RM Code\ntest = Alkalis_df_dummies.copy()\ntest = test.drop(['Time', 'Group Description', 'Year','Month'],axis=1)\nprint(test.groupby('RM02/0002')['Average_price'].mean())","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1710238482421,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Calculate average prices based on Key RM Code\ntest = Alkalis_df_dummies.copy()\ntest = test.drop(['Time', 'Group Description', 'Year','Month'],axis=1)\nprint(test.groupby('RM02/0002')['Average_price'].mean())","outputsMetadata":{"0":{"height":97,"type":"stream"}}},"cell_type":"code","id":"8acf6637-3c74-4e93-8fe8-f9ee62b313ba","outputs":[{"output_type":"stream","name":"stdout","text":"RM02/0002\n0    0.328372\n1    0.647589\nName: Average_price, dtype: float64\n"}],"execution_count":8},{"source":"## Create 12*N features, external factor prices from one-month before to 12-month before\n## Combine features with target variables\n# To prepare feature datasets\nmerged_df = pd.merge(elec_df, TTF_gas_df,how='left', on = (['Year', 'Month', 'Time']))\nfeature_df = merged_df.copy()\nfeature_df['Time_label'] = feature_df['Time'].dt.strftime('%Y-%m')\nfeature_df = feature_df.drop(['Year','Month', 'Time'], axis=1) # to prevent duplicate columns when merging\n\n# create time labels\nlabel_dfs=[]    # To store labels\n                # ref: 'https://pandas.pydata.org/docs/user_guide/merging.html'\n    \nfor i in range(1,13): # 13 is not included\n    label = Alkalis_df_dummies[['Time']]\n    label.rename(columns = {'Time':f'Time_label{i}'}, inplace = True)\n    label = (label[f'Time_label{i}'] - pd.DateOffset(months=i)).dt.strftime('%Y-%m')\n    label_dfs.append(label)\n\nresult = pd.concat(label_dfs, axis=1)\n\n# To merge with features\nfor i in range(1,13): # 13 is not included\n    result = result.merge(feature_df, how='left',\\\n                          left_on=[f'Time_label{i}'],\\\n                          right_on=['Time_label'])\n    result.rename(columns = {'Electricity':f'Electricity_{i}',\n                              'PNGASEUUSDM':f'PNGASEUUSDM_{i}'\n                             }, inplace = True)\n    result = result.drop(['Time_label',f'Time_label{i}'], axis=1)\n    \nAlkalis_df_dummies = pd.concat([Alkalis_df_dummies,result],axis=1)\nprint(Alkalis_df_dummies.info())\nprint(Alkalis_df_dummies)\n\n","metadata":{"executionCancelledAt":null,"executionTime":313,"lastExecutedAt":1710238482734,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Create 12*N features, external factor prices from one-month before to 12-month before\n## Combine features with target variables\n# To prepare feature datasets\nmerged_df = pd.merge(elec_df, TTF_gas_df,how='left', on = (['Year', 'Month', 'Time']))\nfeature_df = merged_df.copy()\nfeature_df['Time_label'] = feature_df['Time'].dt.strftime('%Y-%m')\nfeature_df = feature_df.drop(['Year','Month', 'Time'], axis=1) # to prevent duplicate columns when merging\n\n# create time labels\nlabel_dfs=[]    # To store labels\n                # ref: 'https://pandas.pydata.org/docs/user_guide/merging.html'\n    \nfor i in range(1,13): # 13 is not included\n    label = Alkalis_df_dummies[['Time']]\n    label.rename(columns = {'Time':f'Time_label{i}'}, inplace = True)\n    label = (label[f'Time_label{i}'] - pd.DateOffset(months=i)).dt.strftime('%Y-%m')\n    label_dfs.append(label)\n\nresult = pd.concat(label_dfs, axis=1)\n\n# To merge with features\nfor i in range(1,13): # 13 is not included\n    result = result.merge(feature_df, how='left',\\\n                          left_on=[f'Time_label{i}'],\\\n                          right_on=['Time_label'])\n    result.rename(columns = {'Electricity':f'Electricity_{i}',\n                              'PNGASEUUSDM':f'PNGASEUUSDM_{i}'\n                             }, inplace = True)\n    result = result.drop(['Time_label',f'Time_label{i}'], axis=1)\n    \nAlkalis_df_dummies = pd.concat([Alkalis_df_dummies,result],axis=1)\nprint(Alkalis_df_dummies.info())\nprint(Alkalis_df_dummies)\n\n","outputsMetadata":{"0":{"height":377,"type":"stream"},"1":{"height":217,"type":"stream"}}},"cell_type":"code","id":"af348660-2ae0-4781-bd13-8649ecd482d1","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 6007 entries, 0 to 6006\nData columns (total 30 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   Time               6007 non-null   datetime64[ns]\n 1   Group Description  6007 non-null   object        \n 2   Year               6007 non-null   int64         \n 3   Month              6007 non-null   int64         \n 4   RM02/0002          6007 non-null   uint8         \n 5   Average_price      6007 non-null   float64       \n 6   Electricity_1      6007 non-null   float64       \n 7   PNGASEUUSDM_1      6007 non-null   float64       \n 8   Electricity_2      6007 non-null   float64       \n 9   PNGASEUUSDM_2      6007 non-null   float64       \n 10  Electricity_3      6007 non-null   float64       \n 11  PNGASEUUSDM_3      6007 non-null   float64       \n 12  Electricity_4      6007 non-null   float64       \n 13  PNGASEUUSDM_4      6007 non-null   float64       \n 14  Electricity_5      6007 non-null   float64       \n 15  PNGASEUUSDM_5      6007 non-null   float64       \n 16  Electricity_6      6007 non-null   float64       \n 17  PNGASEUUSDM_6      6007 non-null   float64       \n 18  Electricity_7      6007 non-null   float64       \n 19  PNGASEUUSDM_7      6007 non-null   float64       \n 20  Electricity_8      6007 non-null   float64       \n 21  PNGASEUUSDM_8      6007 non-null   float64       \n 22  Electricity_9      6007 non-null   float64       \n 23  PNGASEUUSDM_9      6007 non-null   float64       \n 24  Electricity_10     6007 non-null   float64       \n 25  PNGASEUUSDM_10     6007 non-null   float64       \n 26  Electricity_11     6007 non-null   float64       \n 27  PNGASEUUSDM_11     6007 non-null   float64       \n 28  Electricity_12     6007 non-null   float64       \n 29  PNGASEUUSDM_12     6007 non-null   float64       \ndtypes: datetime64[ns](1), float64(25), int64(2), object(1), uint8(1)\nmemory usage: 1.4+ MB\nNone\n           Time Group Description  ...  Electricity_12  PNGASEUUSDM_12\n0    2012-01-31           Alkalis  ...           56.66        9.190000\n1    2012-01-31           Alkalis  ...           56.66        9.190000\n2    2012-01-31           Alkalis  ...           56.66        9.190000\n3    2012-01-31           Alkalis  ...           56.66        9.190000\n4    2012-01-31           Alkalis  ...           56.66        9.190000\n...         ...               ...  ...             ...             ...\n6002 2023-10-19           Alkalis  ...          501.92       20.806637\n6003 2023-10-19           Alkalis  ...          501.92       20.806637\n6004 2023-10-11           Alkalis  ...          501.92       20.806637\n6005 2023-10-05           Alkalis  ...          501.92       20.806637\n6006 2023-10-03           Alkalis  ...          501.92       20.806637\n\n[6007 rows x 30 columns]\n"}],"execution_count":9},{"source":"## train_test_split()\n## Log transformation\n\n# # Observe data distribution\n# Alkalis_df_dummies.drop(['RM02/0002','Time', 'Group Description', 'Year','Month'],axis=1).hist()\n# Alkalis_df_dummies['Average_price'].hist()\n\n# Create X, y\nfeature_list = Alkalis_df_dummies.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\nX = feature_list.values\ny = Alkalis_df_dummies['Average_price'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 30% of our data as the test set\n\n# Log transformation - data is highly skewed\npow_trans_x = PowerTransformer()\npow_trans_x.fit(X_train)\nX_train_log = pow_trans_x.transform(X_train)\nX_test_log = pow_trans_x.transform(X_test)\n\npow_trans_y = PowerTransformer()\npow_trans_y.fit(y_train.reshape(-1, 1))  # Reshape y_train before fitting\ny_train_log = pow_trans_y.transform(y_train.reshape(-1, 1))  # Reshape y_train during transformation\ny_test_log = pow_trans_y.transform(y_test.reshape(-1, 1))  # Reshape y_test during transformation\n","metadata":{"executionCancelledAt":null,"executionTime":114,"lastExecutedAt":1710238482848,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## train_test_split()\n## Log transformation\n\n# # Observe data distribution\n# Alkalis_df_dummies.drop(['RM02/0002','Time', 'Group Description', 'Year','Month'],axis=1).hist()\n# Alkalis_df_dummies['Average_price'].hist()\n\n# Create X, y\nfeature_list = Alkalis_df_dummies.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\nX = feature_list.values\ny = Alkalis_df_dummies['Average_price'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 30% of our data as the test set\n\n# Log transformation - data is highly skewed\npow_trans_x = PowerTransformer()\npow_trans_x.fit(X_train)\nX_train_log = pow_trans_x.transform(X_train)\nX_test_log = pow_trans_x.transform(X_test)\n\npow_trans_y = PowerTransformer()\npow_trans_y.fit(y_train.reshape(-1, 1))  # Reshape y_train before fitting\ny_train_log = pow_trans_y.transform(y_train.reshape(-1, 1))  # Reshape y_train during transformation\ny_test_log = pow_trans_y.transform(y_test.reshape(-1, 1))  # Reshape y_test during transformation\n","outputsMetadata":{"0":{"height":237,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"48e7f43f-d67c-4079-a16e-4cc3f57de35e","outputs":[],"execution_count":10},{"source":"## Lasso regression - fit and transform train data set\n## Cross validation and Hyperparameter tuning using RandomizedSearchCV\n\n# Define the parameter grid\nparam_grid = {'alpha': np.linspace(0.0000001, 1, 3000)}\n\n# Create a Lasso regression model\nlasso = Lasso()\n\n# Create RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=lasso, \n                                   # param_grid defines the grid of hyperparameters to search over.\n                                   param_distributions=param_grid, \n                                   # n_iter determines the number samples\n                                   n_iter=300, \n                                   # cv parameter sets the number of folds for cross-validation.\n                                   cv=5, \n                                   random_state=42)\n\n# Fit the data to perform a grid search\nrandom_search.fit(X_train_log, y_train_log)\n\n# Best alpha parameter\nprint(\"Best alpha parameter:\", random_search.best_params_['alpha'])\n\n\n# Best R-squared score\nprint(\"Best R-squared score:\", round(random_search.best_score_, 3))\n\n# Coefficients of the best Lasso model\nassert random_search.n_features_in_ == len(feature_list.columns)\n\nprint(\"Coefficients of the selected features in the best Lasso model:\")\n# zip(X, Y) is a Python built-in function that combines elements from two or more iterables (in this case, lists) into tuples.\nfor feature, coefficient in zip(feature_list.columns, random_search.best_estimator_.coef_):\n    print(f\"{feature}: {round(coefficient,3)}\")","metadata":{"executionCancelledAt":null,"executionTime":24001,"lastExecutedAt":1710238506850,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Lasso regression - fit and transform train data set\n## Cross validation and Hyperparameter tuning using RandomizedSearchCV\n\n# Define the parameter grid\nparam_grid = {'alpha': np.linspace(0.0000001, 1, 3000)}\n\n# Create a Lasso regression model\nlasso = Lasso()\n\n# Create RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=lasso, \n                                   # param_grid defines the grid of hyperparameters to search over.\n                                   param_distributions=param_grid, \n                                   # n_iter determines the number samples\n                                   n_iter=300, \n                                   # cv parameter sets the number of folds for cross-validation.\n                                   cv=5, \n                                   random_state=42)\n\n# Fit the data to perform a grid search\nrandom_search.fit(X_train_log, y_train_log)\n\n# Best alpha parameter\nprint(\"Best alpha parameter:\", random_search.best_params_['alpha'])\n\n\n# Best R-squared score\nprint(\"Best R-squared score:\", round(random_search.best_score_, 3))\n\n# Coefficients of the best Lasso model\nassert random_search.n_features_in_ == len(feature_list.columns)\n\nprint(\"Coefficients of the selected features in the best Lasso model:\")\n# zip(X, Y) is a Python built-in function that combines elements from two or more iterables (in this case, lists) into tuples.\nfor feature, coefficient in zip(feature_list.columns, random_search.best_estimator_.coef_):\n    print(f\"{feature}: {round(coefficient,3)}\")","outputsMetadata":{"0":{"height":377,"type":"stream"}}},"cell_type":"code","id":"b8215f5e-fb58-41ec-a76f-10a7cc271c58","outputs":[{"output_type":"stream","name":"stdout","text":"Best alpha parameter: 1e-07\nBest R-squared score: 0.849\nCoefficients of the selected features in the best Lasso model:\nRM02/0002: 0.481\nElectricity_1: 0.375\nPNGASEUUSDM_1: -0.24\nElectricity_2: 0.086\nPNGASEUUSDM_2: 0.213\nElectricity_3: 0.031\nPNGASEUUSDM_3: -0.178\nElectricity_4: -0.002\nPNGASEUUSDM_4: 0.224\nElectricity_5: 0.104\nPNGASEUUSDM_5: -0.038\nElectricity_6: 0.005\nPNGASEUUSDM_6: 0.017\nElectricity_7: 0.151\nPNGASEUUSDM_7: 0.093\nElectricity_8: -0.017\nPNGASEUUSDM_8: 0.069\nElectricity_9: 0.085\nPNGASEUUSDM_9: -0.236\nElectricity_10: 0.12\nPNGASEUUSDM_10: 0.083\nElectricity_11: 0.15\nPNGASEUUSDM_11: -0.181\nElectricity_12: -0.087\nPNGASEUUSDM_12: 0.092\n"}],"execution_count":11},{"source":"## Lasso regression - transform test data set\n# Get the best Lasso model from RandomizedSearchCV\nbest_lasso_model = random_search.best_estimator_\n\n# Predict on the test data\ny_pred_test = best_lasso_model.predict(X_test_log)\n\n# Evaluate the model performance on the test data\ntest_score = best_lasso_model.score(X_test_log, y_test_log)\nprint(\"Best Model:\", best_lasso_model)\nprint(\"Test Set R-squared score:\", round(test_score, 3))","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1710238506900,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"## Lasso regression - transform test data set\n# Get the best Lasso model from RandomizedSearchCV\nbest_lasso_model = random_search.best_estimator_\n\n# Predict on the test data\ny_pred_test = best_lasso_model.predict(X_test_log)\n\n# Evaluate the model performance on the test data\ntest_score = best_lasso_model.score(X_test_log, y_test_log)\nprint(\"Best Model:\", best_lasso_model)\nprint(\"Test Set R-squared score:\", round(test_score, 3))","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"cell_type":"code","id":"49a8e9d1-f461-4266-981f-9edd99e8fd77","outputs":[{"output_type":"stream","name":"stdout","text":"Best Model: Lasso(alpha=1e-07)\nTest Set R-squared score: 0.847\n"}],"execution_count":12}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}